{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import Workbook\n",
    "\n",
    "TOKEN = \"hf_xxxxxxxxxxxxxxxxxxxxx\"\n",
    "login(token=TOKEN)\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful guide chatbot. Follow these rules strictly:\n",
    "1. Answer only based on the information you were trained on\n",
    "2. If you're not sure or don't have specific information, say \"I don't have specific information about that\"\n",
    "3. Provide a complete response in 1-2 full sentences that end with a period\n",
    "4. Keep your response clear, direct, and between 20-50 words\n",
    "5. Focus only on answering what was asked\n",
    "6. Do not make assumptions or create information that wasn't in your training data\n",
    "7. For businesses: only provide information that you are confident was in your training data\"\"\"\n",
    "\n",
    "questions = [\n",
    "    'Tell us about Copper Vine Nightlift pub?',\n",
    "    'In CVS Pharmacy, what can I buy?',\n",
    "    'Is there a good pizza place?',\n",
    "    \"I am going to MacAlister's Deli and want to know what are the good things to order?\",\n",
    "]\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", token=TOKEN)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", token=TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/kaggle/input/final-restaurant-qa-model\",\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "def clean_response(response):\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[-1].strip()\n",
    "    response = response.replace(\"- \", \"\").replace(\"\\n\", \" \").strip()\n",
    "    if not response.endswith('.'):\n",
    "        response += '.'\n",
    "    return response\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"{i}/{len(questions)}\")\n",
    "    \n",
    "    formatted_prompt = f\"{system_prompt}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,      \n",
    "        min_new_tokens=20,      \n",
    "        temperature=0.3,        \n",
    "        do_sample=True,\n",
    "        num_beams=3,          \n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2 \n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    cleaned_response = clean_response(response)\n",
    "    \n",
    "    results.append({\n",
    "        'question_id': i,\n",
    "        'question': question,\n",
    "        'response': cleaned_response,\n",
    "        'score': ''\n",
    "    })\n",
    "\n",
    "filename = f'restaurant_qa_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(filename, index=False, engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6304709,
     "sourceId": 10202293,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
